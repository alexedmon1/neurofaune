#!/usr/bin/env python3
"""
Slice-Level QC for TBSS Analysis

Handles the rodent-specific problem of bad slices in partial-coverage DTI.
Creates validity masks and supports imputation for missing data.

Strategy B (Threshold-Based): Preserves subjects with isolated bad slices
by masking out only the affected voxels and imputing with group mean.

Usage:
    # After prepare_tbss.py, before run_tbss_stats.py:
    from neurofaune.analysis.tbss.slice_qc import apply_slice_masking

    result = apply_slice_masking(
        tbss_dir=Path('/study/analysis/tbss'),
        slice_exclusions_file=Path('/study/qc/dwi_batch_summary/slice_qc/slice_exclusions.json'),
        min_valid_fraction=0.8
    )
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import nibabel as nib
import numpy as np


def create_validity_masks_from_batch_qc(
    slice_exclusions_file: Path,
    tbss_dir: Path,
    output_dir: Path
) -> Dict[str, Path]:
    """
    Create per-subject validity masks from batch QC slice exclusions.

    Reads the slice exclusion data generated by the batch QC system
    and creates 3D binary masks in SIGMA space where valid slices = 1.

    Args:
        slice_exclusions_file: Path to slice_exclusions.json from batch QC
        tbss_dir: TBSS output directory (contains warped FA files)
        output_dir: Directory to save validity masks

    Returns:
        Dict mapping subject_session -> validity_mask_path
    """
    logger = logging.getLogger("neurofaune.tbss")
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    with open(slice_exclusions_file) as f:
        slice_data = json.load(f)

    # Get reference image dimensions from any warped FA file
    fa_dir = tbss_dir / 'FA'
    fa_files = sorted(fa_dir.glob('*_FA_sigma.nii.gz'))
    if not fa_files:
        raise FileNotFoundError(f"No warped FA files found in {fa_dir}")

    ref_img = nib.load(fa_files[0])
    shape = ref_img.shape[:3]
    affine = ref_img.affine

    # Load subject list to match order
    subject_list_file = tbss_dir / 'subject_list.txt'
    if not subject_list_file.exists():
        raise FileNotFoundError(f"Subject list not found: {subject_list_file}")

    with open(subject_list_file) as f:
        subject_list = [line.strip() for line in f if line.strip()]

    # Build exclusion lookup
    exclusions = slice_data.get('exclusions', {})

    validity_masks = {}
    n_with_bad_slices = 0

    for subject_session in subject_list:
        mask = np.ones(shape, dtype=np.uint8)

        if subject_session in exclusions:
            bad_slices = exclusions[subject_session].get('bad_slices', [])
            if bad_slices:
                n_with_bad_slices += 1
                for s in bad_slices:
                    if 0 <= s < shape[2]:
                        mask[:, :, s] = 0
                logger.info(
                    f"  {subject_session}: {len(bad_slices)} bad slices "
                    f"({shape[2] - len(bad_slices)}/{shape[2]} valid)"
                )

        mask_file = output_dir / f'{subject_session}_validity.nii.gz'
        nib.save(nib.Nifti1Image(mask, affine), mask_file)
        validity_masks[subject_session] = mask_file

    logger.info(f"Created {len(validity_masks)} validity masks "
                f"({n_with_bad_slices} subjects with bad slices)")

    return validity_masks


def create_group_validity_map(
    validity_masks: Dict[str, Path],
    output_dir: Path,
    min_valid_fraction: float = 0.8
) -> Tuple[Path, Path]:
    """
    Create group-level validity map showing per-voxel subject coverage.

    Args:
        validity_masks: Dict mapping subject_session -> validity_mask_path
        output_dir: Output directory
        min_valid_fraction: Minimum fraction of subjects required (default: 0.8)

    Returns:
        Tuple of (valid_count_map, analysis_mask) paths
    """
    logger = logging.getLogger("neurofaune.tbss")
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    mask_files = list(validity_masks.values())
    if not mask_files:
        raise ValueError("No validity masks provided")

    ref_img = nib.load(mask_files[0])
    shape = ref_img.shape
    affine = ref_img.affine
    n_subjects = len(mask_files)

    # Sum validity across subjects
    valid_count = np.zeros(shape, dtype=np.int32)
    for mask_file in mask_files:
        mask_data = nib.load(mask_file).get_fdata()
        valid_count += (mask_data > 0).astype(np.int32)

    # Create analysis mask: voxels where >= min_valid_fraction subjects are valid
    min_subjects = int(np.ceil(min_valid_fraction * n_subjects))
    analysis_mask = (valid_count >= min_subjects).astype(np.uint8)

    # Save outputs
    count_file = output_dir / 'group_validity_map.nii.gz'
    mask_file = output_dir / f'analysis_mask_frac{min_valid_fraction:.1f}.nii.gz'

    nib.save(nib.Nifti1Image(valid_count, affine), count_file)
    nib.save(nib.Nifti1Image(analysis_mask, affine), mask_file)

    n_included = int(np.sum(analysis_mask))
    n_total = int(np.sum(valid_count > 0))
    logger.info(
        f"Group validity: {n_included}/{n_total} voxels pass "
        f"threshold (>={min_subjects}/{n_subjects} subjects)"
    )

    return count_file, mask_file


def prepare_data_with_missing(
    skeletonised_4d: Path,
    validity_masks: Dict[str, Path],
    skeleton_mask: Path,
    min_valid_fraction: float = 0.8,
    output_dir: Optional[Path] = None
) -> Tuple[Path, Path]:
    """
    Prepare skeletonised data with missing slice handling.

    For voxels where a subject has invalid data (bad slice), imputes
    with the group mean. Creates an analysis mask excluding voxels
    where too few subjects have valid data.

    Args:
        skeletonised_4d: 4D skeletonised metric (subjects Ã— voxels)
        validity_masks: Dict mapping subject_session -> validity_mask_path
        skeleton_mask: Binary skeleton mask
        min_valid_fraction: Minimum valid fraction for inclusion
        output_dir: Output directory (default: same as skeletonised_4d parent)

    Returns:
        Tuple of (imputed_4d, analysis_mask) paths
    """
    logger = logging.getLogger("neurofaune.tbss")

    if output_dir is None:
        output_dir = Path(skeletonised_4d).parent
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load skeletonised data
    skel_img = nib.load(skeletonised_4d)
    skel_data = skel_img.get_fdata().copy()
    affine = skel_img.affine
    n_subjects = skel_data.shape[3]

    # Load skeleton mask
    skel_mask_data = nib.load(skeleton_mask).get_fdata() > 0

    # Load validity masks in subject order
    subject_list_file = Path(skeletonised_4d).parent.parent / 'subject_list.txt'
    if subject_list_file.exists():
        with open(subject_list_file) as f:
            subject_order = [line.strip() for line in f if line.strip()]
    else:
        subject_order = sorted(validity_masks.keys())

    if len(subject_order) != n_subjects:
        raise ValueError(
            f"Subject list ({len(subject_order)}) doesn't match "
            f"4D data ({n_subjects} volumes)"
        )

    # Build per-subject validity in skeleton space
    validity_3d = np.ones((*skel_data.shape[:3], n_subjects), dtype=bool)
    for i, subject_session in enumerate(subject_order):
        if subject_session in validity_masks:
            v_data = nib.load(validity_masks[subject_session]).get_fdata()
            validity_3d[..., i] = v_data > 0

    # Count valid subjects per voxel (within skeleton)
    valid_count = np.sum(validity_3d, axis=3)
    min_subjects = int(np.ceil(min_valid_fraction * n_subjects))

    # Analysis mask: skeleton AND enough valid subjects
    analysis_mask = skel_mask_data & (valid_count >= min_subjects)

    # Impute missing values with group mean
    n_imputed = 0
    for i in range(n_subjects):
        invalid_voxels = analysis_mask & ~validity_3d[..., i]
        if np.any(invalid_voxels):
            # Compute mean from valid subjects at these voxels
            valid_subjects_mask = validity_3d[invalid_voxels]
            for vox_idx in np.argwhere(invalid_voxels):
                x, y, z = vox_idx
                valid_vals = skel_data[x, y, z, validity_3d[x, y, z]]
                if len(valid_vals) > 0:
                    skel_data[x, y, z, i] = np.mean(valid_vals)
                else:
                    skel_data[x, y, z, i] = 0.0
            n_imputed += int(np.sum(invalid_voxels))

    # Save outputs
    stem = Path(skeletonised_4d).name.replace('.nii.gz', '')
    imputed_file = output_dir / f'{stem}_imputed.nii.gz'
    mask_file = output_dir / f'{stem}_analysis_mask.nii.gz'

    nib.save(nib.Nifti1Image(skel_data, affine), imputed_file)
    nib.save(nib.Nifti1Image(analysis_mask.astype(np.uint8), affine), mask_file)

    logger.info(
        f"Imputation: {n_imputed} voxel-subject pairs imputed with group mean"
    )
    logger.info(
        f"Analysis mask: {int(np.sum(analysis_mask))} voxels "
        f"(>= {min_subjects}/{n_subjects} valid subjects)"
    )

    return imputed_file, mask_file


def apply_slice_masking(
    tbss_dir: Path,
    slice_exclusions_file: Path,
    min_valid_fraction: float = 0.8,
    metrics: List[str] = None
) -> Dict:
    """
    Apply slice-level masking to prepared TBSS data.

    This is the main entry point for slice QC integration.
    Should be run after prepare_tbss.py and before run_tbss_stats.py.

    Args:
        tbss_dir: TBSS output directory from prepare_tbss.py
        slice_exclusions_file: Path to slice_exclusions.json from batch QC
        min_valid_fraction: Minimum valid fraction (default: 0.8)
        metrics: Metrics to process (default: ['FA', 'MD', 'AD', 'RD'])

    Returns:
        Dict with paths to imputed data and analysis masks
    """
    logger = logging.getLogger("neurofaune.tbss")

    if metrics is None:
        metrics = ['FA', 'MD', 'AD', 'RD']

    tbss_dir = Path(tbss_dir)
    slice_qc_dir = tbss_dir / 'slice_qc'

    logger.info("=" * 80)
    logger.info("Applying Slice-Level Masking for TBSS")
    logger.info("=" * 80)
    logger.info(f"TBSS dir: {tbss_dir}")
    logger.info(f"Exclusions: {slice_exclusions_file}")
    logger.info(f"Min valid fraction: {min_valid_fraction}")

    # Step 1: Create validity masks
    logger.info("\n[Step 1] Creating per-subject validity masks...")
    validity_dir = slice_qc_dir / 'validity_masks'
    validity_masks = create_validity_masks_from_batch_qc(
        slice_exclusions_file=slice_exclusions_file,
        tbss_dir=tbss_dir,
        output_dir=validity_dir
    )

    # Step 2: Create group validity map
    logger.info("\n[Step 2] Creating group validity map...")
    group_count, group_mask = create_group_validity_map(
        validity_masks=validity_masks,
        output_dir=slice_qc_dir,
        min_valid_fraction=min_valid_fraction
    )

    # Step 3: Impute missing data for each metric
    logger.info("\n[Step 3] Imputing missing data...")
    results = {
        'validity_masks_dir': str(validity_dir),
        'group_validity_map': str(group_count),
        'analysis_mask': str(group_mask),
        'imputed_files': {},
        'analysis_masks': {}
    }

    stats_dir = tbss_dir / 'stats'
    skeleton_mask = stats_dir / 'mean_FA_skeleton_mask.nii.gz'

    for metric in metrics:
        skel_file = stats_dir / f'all_{metric}_skeletonised.nii.gz'
        if not skel_file.exists():
            logger.warning(f"Skeletonised {metric} not found, skipping")
            continue

        logger.info(f"\n  Processing {metric}...")
        imputed, mask = prepare_data_with_missing(
            skeletonised_4d=skel_file,
            validity_masks=validity_masks,
            skeleton_mask=skeleton_mask,
            min_valid_fraction=min_valid_fraction,
            output_dir=slice_qc_dir
        )

        results['imputed_files'][metric] = str(imputed)
        results['analysis_masks'][metric] = str(mask)

    # Save summary
    summary_file = slice_qc_dir / 'validity_report.json'
    with open(summary_file, 'w') as f:
        json.dump(results, f, indent=2)

    logger.info("\n" + "=" * 80)
    logger.info("Slice masking complete")
    logger.info(f"Use imputed files for statistical analysis:")
    for metric, path in results['imputed_files'].items():
        logger.info(f"  {metric}: {path}")
    logger.info("=" * 80)

    return results


def generate_slice_qc_heatmap(
    slice_exclusions_file: Path,
    output_file: Path,
    n_slices: int = 11
) -> Path:
    """
    Generate a visual heatmap of slice quality across subjects.

    Creates a subjects x slices heatmap showing good/bad slices.

    Args:
        slice_exclusions_file: Path to slice_exclusions.json
        output_file: Output PNG path
        n_slices: Number of slices per subject (default: 11 for DTI)

    Returns:
        Path to generated heatmap
    """
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
    except ImportError:
        logging.getLogger("neurofaune.tbss").warning(
            "matplotlib not available, skipping heatmap generation"
        )
        return output_file

    with open(slice_exclusions_file) as f:
        slice_data = json.load(f)

    exclusions = slice_data.get('exclusions', {})
    subjects = sorted(exclusions.keys())

    if not subjects:
        return output_file

    # Build quality matrix
    quality = np.ones((len(subjects), n_slices))
    for i, subj in enumerate(subjects):
        bad_slices = exclusions[subj].get('bad_slices', [])
        for s in bad_slices:
            if 0 <= s < n_slices:
                quality[i, s] = 0

    fig, ax = plt.subplots(figsize=(8, max(4, len(subjects) * 0.3)))
    ax.imshow(quality, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1,
              interpolation='nearest')
    ax.set_xlabel('Slice Index')
    ax.set_ylabel('Subject')
    ax.set_title(f'Slice Quality ({len(subjects)} subjects with issues)')
    ax.set_xticks(range(n_slices))

    if len(subjects) <= 30:
        ax.set_yticks(range(len(subjects)))
        ax.set_yticklabels(subjects, fontsize=6)

    plt.tight_layout()
    output_file.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_file, dpi=150)
    plt.close()

    return output_file
